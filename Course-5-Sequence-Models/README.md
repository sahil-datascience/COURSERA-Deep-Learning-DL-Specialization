# Course 5: Sequence Models

This course teaches you how to build models for natural language, audio, and other sequence data.

## üìã Course Overview

This course will teach you how to build models for natural language, audio, and other sequence data. You will learn to use recurrent neural networks (RNNs) and their variants, including LSTMs and GRUs, as well as how to apply sequence models to natural language processing tasks.

## üóÇÔ∏è Weekly Structure

### Week 1: Recurrent Neural Networks
- Why sequence models?
- Notation
- Recurrent Neural Network Model
- Backpropagation through time
- Different types of RNNs
- Language model and sequence generation
- Sampling novel sequences
- Vanishing gradients with RNNs
- Gated Recurrent Unit (GRU)
- Long Short Term Memory (LSTM)
- Bidirectional RNN
- Deep RNNs

**Programming Assignments**:
- Building a Recurrent Neural Network - Step by Step
- Dinosaur Island -- Character-level language model
- Improvise a Jazz Solo with an LSTM Network

### Week 2: Natural Language Processing & Word Embeddings
- Word Representation
- Using word embeddings
- Properties of word embeddings
- Embedding matrix
- Learning word embeddings
- Word2Vec
- Negative Sampling
- GloVe word vectors
- Sentiment Classification
- Debiasing word embeddings

**Programming Assignments**:
- Operations on word vectors
- Emojify

### Week 3: Sequence Models & Attention Mechanism
- Various sequence to sequence architectures
- Basic Models
- Picking the most likely sentence
- Beam Search
- Refinements to Beam Search
- Error analysis in beam search
- Bleu Score
- Attention Model Intuition
- Attention Model
- Speech recognition - Audio data
- CTC cost for speech recognition
- Trigger Word Detection

**Programming Assignments**:
- Neural machine translation with attention
- Trigger word detection

## üéØ Learning Outcomes

By the end of this course, you will be able to:
- Build and train RNNs, including LSTMs and GRUs
- Apply sequence models to natural language processing problems
- Use word embeddings and understand their properties
- Build attention models for sequence-to-sequence tasks
- Apply sequence models to speech recognition and trigger word detection

## üìÅ Assignment Files

- `Week-1/`: RNN fundamentals, character-level language models, and jazz generation
- `Week-2/`: Word embeddings, Word2Vec, and sentiment classification
- `Week-3/`: Sequence-to-sequence models, attention mechanisms, and trigger word detection

## üß† Key Architectures Covered

- **Vanilla RNN**: Basic recurrent neural network
- **LSTM**: Long Short-Term Memory networks
- **GRU**: Gated Recurrent Units
- **Bidirectional RNN**: Processing sequences in both directions
- **Deep RNN**: Stacked recurrent layers
- **Encoder-Decoder**: Sequence-to-sequence architecture
- **Attention Model**: Focus mechanism for long sequences