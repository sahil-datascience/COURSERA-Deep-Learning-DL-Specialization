# Course 2: Improving Deep Neural Networks: Hyperparameter Tuning, Regularization and Optimization

This course teaches you the practical aspects of deep learning and how to make your neural networks work well.

## üìã Course Overview

This course will teach you the "magic" of getting deep learning to work well. You'll learn about regularization, optimization algorithms, hyperparameter tuning, and how to build a machine learning project from start to finish.

## üóÇÔ∏è Weekly Structure

### Week 1: Practical Aspects of Deep Learning
- Setting up your Machine Learning Application
- Train/Dev/Test sets
- Bias/Variance
- Basic Recipe for Machine Learning
- Regularization
- Why regularization reduces overfitting?
- Dropout Regularization
- Understanding Dropout
- Other regularization methods
- Normalizing inputs
- Vanishing/Exploding gradients
- Weight Initialization for Deep Networks
- Numerical approximation of gradients
- Gradient checking and implementation

**Programming Assignments**:
- Initialization
- Regularization
- Gradient Checking

### Week 2: Optimization Algorithms
- Mini-batch gradient descent
- Understanding mini-batch gradient descent
- Exponentially weighted averages
- Understanding exponentially weighted averages
- Bias correction in exponentially weighted averages
- Gradient descent with momentum
- RMSprop
- Adam optimization algorithm
- Learning rate decay
- The problem of local optima

**Programming Assignment**: Optimization Methods

### Week 3: Hyperparameter Tuning, Batch Normalization and Programming Frameworks
- Tuning process
- Using an appropriate scale to pick hyperparameters
- Hyperparameters tuning in practice: Pandas vs. Caviar
- Normalizing activations in a network
- Fitting Batch Normalization into a neural network
- Why does Batch Normalization work?
- Batch Normalization at test time
- Softmax Regression
- Training a Softmax classifier
- Deep learning frameworks
- TensorFlow

**Programming Assignment**: TensorFlow Tutorial

## üéØ Learning Outcomes

By the end of this course, you will be able to:
- Use various optimization algorithms to train neural networks
- Apply regularization techniques to avoid overfitting
- Tune hyperparameters effectively
- Use TensorFlow to build deep learning models

## üìÅ Assignment Files

- `Week-1/`: Initialization, Regularization, and Gradient Checking
- `Week-2/`: Optimization algorithms (Momentum, RMSprop, Adam)
- `Week-3/`: TensorFlow implementation tutorial